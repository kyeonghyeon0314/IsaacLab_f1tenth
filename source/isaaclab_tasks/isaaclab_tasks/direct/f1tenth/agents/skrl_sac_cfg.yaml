# SAC configuration for F1TENTH racing
# Based on SKRL SAC implementation
# https://skrl.readthedocs.io/en/latest/api/agents/sac.html

seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
models:
  separate: True  # SAC requires separate models (policy, critic_1, critic_2)

  policy:  # Actor network
    class: GaussianMixin
    clip_actions: True
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0
    network:
      - name: net
        input: STATES  # LiDAR + vehicle state
        layers: [512, 512, 256]
        activations: relu
    output: ACTIONS

  critic_1:  # First Q-network
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES_ACTIONS  # skrl special keyword: auto-concatenates states and actions
        layers: [512, 512, 256]
        activations: relu
    output: ONE
  critic_2:  # Second Q-network (for double Q-learning)
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES_ACTIONS  # skrl special keyword: auto-concatenates states and actions
        layers: [512, 512, 256]
        activations: relu
    output: ONE

# SAC agent configuration
# https://skrl.readthedocs.io/en/latest/api/agents/sac.html
agent:
  class: SAC

  # Experiment configuration
  experiment:
    directory: f1tenth_sac
    experiment_name: ""
    write_interval: 1000
    checkpoint_interval: 10000

  rollouts: 16  # Number of rollouts before updating

  # Discount factor for future rewards
  discount_factor: 0.99

  # Polyak averaging factor for target networks
  polyak: 0.005

  # Learning rates
  learning_rate: 3.0e-4
  learning_rate_scheduler: KLAdaptiveLR
  learning_rate_scheduler_kwargs:
    kl_threshold: 0.008

  # State preprocessing
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs:
    size: 1086  # LiDAR(1080) + vehicle_state(6)
    epsilon: 1.0e-8

  # Reward scaling
  value_preprocessor: RunningStandardScaler
  value_preprocessor_kwargs: null

  # Random timesteps for exploration
  random_timesteps: 10000

  # Learning starts after this many steps
  learning_starts: 10000

  # Gradient steps per update
  grad_norm_clip: 1.0

  # Batch size for training
  batch_size: 4096

  # Entropy regularization
  learn_entropy: True
  entropy_learning_rate: 3.0e-4
  initial_entropy_value: 0.2
  target_entropy: null  # Auto-tune based on action space

  # Rewards shaping
  rewards_shaper: null

  # Replay buffer
  replay_buffer_size: 1000000
  replay_buffer_class: RandomMemory
  replay_buffer_kwargs: null

# Training configuration
trainer:
  class: SequentialTrainer

  timesteps: 100000  # Total training timesteps

  environment_info: log  # Log environment info

  # Checkpoint settings
  write_interval: 1000  # Save every 1000 timesteps
  checkpoint_interval: 10000  # Checkpoint every 10000 timesteps

  # TensorBoard logging
  wandb: False
  wandb_kwargs: {}

# Evaluation (optional)
eval_interval: 5000  # Evaluate every 5000 steps
eval_episodes: 10

# Hardware
device: cuda:0

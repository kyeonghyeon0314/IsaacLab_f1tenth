# SAC configuration for F1TENTH racing
# Based on SKRL SAC implementation
# https://skrl.readthedocs.io/en/latest/api/agents/sac.html

seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
models:
  separate: True  # SAC requires separate models (policy, critic_1, critic_2)

  policy:  # Actor network
    class: GaussianMixin
    clip_actions: True
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0
    network:
      - name: net
        input: STATES  # LiDAR + vehicle state
        layers: [512, 512, 256]
        activations: relu
    output: ACTIONS

  critic_1:  # First Q-network
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES_ACTIONS  # skrl special keyword: auto-concatenates states and actions
        layers: [512, 512, 256]
        activations: relu
    output: ONE
  critic_2:  # Second Q-network (for double Q-learning)
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES_ACTIONS  # skrl special keyword: auto-concatenates states and actions
        layers: [512, 512, 256]
        activations: relu
    output: ONE

# SAC agent configuration
# https://skrl.readthedocs.io/en/latest/api/agents/sac.html
agent:
  class: SAC

  # Experiment configuration
  experiment:
    directory: f1tenth_sac
    experiment_name: ""
    write_interval: 1000
    checkpoint_interval: 10000

  # SAC-specific: gradient steps per environment interaction
  gradient_steps: 1  # Number of gradient updates per step (SAC default)

  # Memory/rollouts: SAC doesn't use rollouts but skrl Runner requires it for memory_size calculation
  # This is a workaround for skrl's internal memory allocation
  rollouts: 1  # Dummy value (SAC uses replay buffer instead)

  # Discount factor for future rewards
  discount_factor: 0.99

  # Polyak averaging factor for target networks
  polyak: 0.005

  # Learning rates (SAC typically uses fixed learning rate)
  learning_rate: 3.0e-4
  learning_rate_scheduler: null  # SAC doesn't need PPO's KLAdaptiveLR
  learning_rate_scheduler_kwargs: {}

  # State preprocessing
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs:
    size: 1082  # LiDAR(1080) + vehicle_state(2: speed + steering)
    epsilon: 1.0e-8

  # Reward scaling
  value_preprocessor: RunningStandardScaler
  value_preprocessor_kwargs: null

  # Random timesteps for exploration
  random_timesteps: 500  # Start small for debugging

  # Learning starts after this many steps
  learning_starts: 1000

  # Gradient steps per update
  grad_norm_clip: 1.0

  # Batch size for training (scale with num_envs)
  # num_envs=1 → 128, num_envs=512 → 2048, num_envs=1024 → 4096
  batch_size: 2048

  # Entropy regularization
  learn_entropy: True
  entropy_learning_rate: 3.0e-4
  initial_entropy_value: 0.2
  target_entropy: null  # Auto-tune based on action space

  # Rewards shaping
  rewards_shaper: null


# Replay buffer for SAC
memory:
  class: RandomMemory
  memory_size: 1000000


# Training configuration
trainer:
  class: SequentialTrainer

  timesteps: 50000  # Total training timesteps

  environment_info: log  # Log environment info

  # Checkpoint settings
  write_interval: 500  # Save every 500 timesteps
  checkpoint_interval: 5000  # Checkpoint every 10000 timesteps

  # TensorBoard logging
  wandb: False
  wandb_kwargs: {}

# Evaluation (optional)
eval_interval: 5000  # Evaluate every 5000 steps
eval_episodes: 10

# Hardware
device: cuda:3  # Changed from cuda:0 to use GPU 3
